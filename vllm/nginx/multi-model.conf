# Upstream definitions for each model backend
upstream gpt-oss-20b-backend {
    server vllm-gpt-oss-20b:8000;
    keepalive 32;
}

upstream gpt-oss-120b-backend {
    server vllm-gpt-oss-120b:8000;
    keepalive 32;
}

upstream qwen-30b-backend {
    server vllm-qwen-30b:8000;
    keepalive 32;
}

upstream litellm-backend {
    server litellm:4000;
    keepalive 32;
}

# HTTPS server
server {
    listen 80;
    listen 443 ssl http2;
    server_name _;

    ssl_certificate /etc/nginx/ssl/public.pem;
    ssl_certificate_key /etc/nginx/ssl/private.pem;

    # Secure SSL settings
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 1h;

    # Route to GPT-OSS-20B
    location /gpt-oss-20b/ {
        rewrite ^/gpt-oss-20b/(.*) /$1 break;

        # Long timeouts for tool calling and reasoning
        proxy_read_timeout 1800;
        proxy_connect_timeout 1800;
        proxy_send_timeout 1800;

        # Streaming optimization
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";

        proxy_pass http://gpt-oss-20b-backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

        # Route to GPT-OSS-120B
    location /gpt-oss-120b/ {
        rewrite ^/gpt-oss-120b/(.*) /$1 break;

        # Long timeouts for tool calling and reasoning
        proxy_read_timeout 1800;
        proxy_connect_timeout 1800;
        proxy_send_timeout 1800;

        # Streaming optimization
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";

        proxy_pass http://gpt-oss-120b-backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Route to Qwen-30B
    location /qwen-30b/ {
        rewrite ^/qwen-30b/(.*) /$1 break;

        # Long timeouts for tool calling and reasoning
        proxy_read_timeout 1800;
        proxy_connect_timeout 1800;
        proxy_send_timeout 1800;

        # Streaming optimization
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";

        proxy_pass http://qwen-30b-backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Route to LiteLLM Proxy (unified API for all models)
    location /litellm/ {
        rewrite ^/litellm/(.*) /$1 break;

        # Long timeouts for tool calling and reasoning
        proxy_read_timeout 1800;
        proxy_connect_timeout 1800;
        proxy_send_timeout 1800;

        # Streaming optimization
        proxy_buffering off;
        proxy_cache off;
        proxy_http_version 1.1;
        proxy_set_header Connection "";

        proxy_pass http://litellm-backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Pass through authorization header for LiteLLM
        proxy_set_header Authorization $http_authorization;
    }

    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }

    # API endpoint to list available models
    location = /v1/models {
        default_type application/json;
        return 200 '{
  "object": "list",
  "data": [
    {
      "id": "gpt-oss-20b",
      "object": "model",
      "endpoint": "/gpt-oss-20b/v1/chat/completions"
    },
    {
      "id": "gpt-oss-120b",
      "object": "model",
      "endpoint": "/gpt-oss-120b/v1/chat/completions"
    },
    {
      "id": "qwen-30b",
      "object": "model",
      "endpoint": "/qwen-30b/v1/chat/completions"
    },
    {
      "id": "litellm",
      "object": "proxy",
      "endpoint": "/litellm/v1/chat/completions",
      "description": "Unified LiteLLM proxy with load balancing and caching"
    }
  ]
}';
    }

    # Default route - show available endpoints
    location / {
        default_type application/json;
        return 200 '{
  "service": "vLLM Multi-Model Gateway with LiteLLM",
  "models": ["/gpt-oss-20b", "/gpt-oss-120b", "/qwen-30b"],
  "proxy": "/litellm",
  "endpoints": {
    "gpt-oss-20b": "https://<your-host>/gpt-oss-20b/v1/chat/completions",
    "gpt-oss-120b": "https://<your-host>/gpt-oss-120b/v1/chat/completions",
    "qwen-30b": "https://<your-host>/qwen-30b/v1/chat/completions",
    "litellm": "https://<your-host>/litellm/v1/chat/completions",
    "litellm-ui": "https://<your-host>/litellm/ui"
  }
}';
    }
}

# HTTP server - redirect to HTTPS
server {
    listen 80;
    server_name _;

    # Health check on HTTP (for load balancers)
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }

    # Redirect all other traffic to HTTPS
    location / {
        return 301 https://$host$request_uri;
    }
}
