upstream litellm-backend {
    server vllm-litellm-proxy:4000;
    keepalive 32;
}

server {
    listen 80;
    listen 443 ssl http2;
    server_name localhost;
    #server_name ai.fabric-testbed.net;

    ssl_certificate /etc/nginx/ssl/public.pem;
    ssl_certificate_key /etc/nginx/ssl/private.pem;

    # Optimization for LLM streaming
    proxy_buffering off;
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    proxy_read_timeout 1800;

    location / {
        proxy_pass http://litellm-backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Helps the UI understand it is behind an SSL proxy
        proxy_set_header X-Forwarded-Prefix /;
    }
}