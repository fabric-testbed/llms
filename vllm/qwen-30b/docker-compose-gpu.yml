services:
  vllm-qwen-30b:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen-30b
    hostname: vllm-qwen-30b

    # Expose port for direct testing
    #ports:
    #  - "8002:8000"

    environment:
      # DGX Spark optimizations
      - CUDA_MANAGED_FORCE_DEVICE_ALLOC=1
      - CUDA_VISIBLE_DEVICES=0,1,2
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - VLLM_LOGGING_LEVEL=${VLLM_LOGGING_LEVEL:-INFO}
      - VLLM_USE_V1=1

      # HuggingFace token for gated models
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/root/.cache/huggingface

    volumes:
      # Shared HuggingFace cache
      - huggingface_cache:/root/.cache/huggingface
      - ./chat.template:/root/chat.template

    # GPU configuration - assign specific GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1', '2']
              capabilities: [gpu]

    shm_size: 16gb
    ipc: host

    ulimits:
      memlock: -1
      stack: 67108864

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # 5 minutes for model loading

    command: >
      --model Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8
      --enable-prefix-caching
      --max-num-seqs 128
      --gpu-memory-utilization 0.95
      --download-dir /root/.cache/huggingface
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
      --chat-template /root/chat.template
      --tensor-parallel-size 2
      --trust-remote-code
      --served-model-name qwen3-coder-30b

    # Join shared network for nginx
    networks:
      - vllm_network

volumes:
  huggingface_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/huggingface

networks:
  vllm_network:
    name: vllm_network
    driver: bridge
    external: true  # Network created by first model
