services:
  vllm-gpt-oss-120b:
    image: vllm/vllm-openai:latest
    container_name: vllm-gpt-oss-120b
    hostname: vllm-gpt-oss-120b

    # Expose port for direct testing
    #ports:
    #  - "8002:8000"

    environment:
      # DGX Spark optimizations
      - CUDA_MANAGED_FORCE_DEVICE_ALLOC=1
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - VLLM_LOGGING_LEVEL=${VLLM_LOGGING_LEVEL:-INFO}
      - VLLM_USE_V1=1

      # HuggingFace token for gated models
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/root/.cache/huggingface

    volumes:
      # Shared HuggingFace cache
      - huggingface_cache:/root/.cache/huggingface
      - ./chat.template:/root/chat.template

    # GPU configuration - assign specific GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1', '2']
              capabilities: [gpu]

    shm_size: 16gb
    ipc: host

    ulimits:
      memlock: -1
      stack: 67108864

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # 5 minutes for model loading

    command: >
      --model openai/gpt-oss-120b
      --enable-auto-tool-choice
      --tool-call-parser openai
      --chat-template /root/chat.template
      --tensor-parallel-size 3
      --trust-remote-code

    # Join shared network for nginx
    networks:
      - vllm_network

volumes:
  huggingface_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/huggingface

networks:
  vllm_network:
    name: vllm_network
    driver: bridge
